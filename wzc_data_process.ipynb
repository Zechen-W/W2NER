{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "data = json.load(open('./output.json'))\n",
    "json.dump(data, open('./output.json', 'w'), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 生成EE的数据集\n",
    "import numpy as np\n",
    "import json\n",
    "data = json.load(open('./data/ee_convert/before.json'))\n",
    "to_save=[]\n",
    "for dialog in data:\n",
    "    for turn in dialog['content']:\n",
    "        keys = list(turn.keys())[:2]\n",
    "        # t_ents = turn['info']['ents']\n",
    "        sentence = ' '.join(turn[keys[0]]).split()\n",
    "        item1={\n",
    "            'sentence': sentence,\n",
    "            'ner':[]\n",
    "        }\n",
    "\n",
    "        sentence = ' '.join(turn[keys[1]]).split()\n",
    "        # for i, char in enumerate(sentence):\n",
    "        #     if char == '|':\n",
    "        #         sentence[i]=' '\n",
    "        item2={\n",
    "            'sentence':sentence,\n",
    "            'ner':[]\n",
    "        }\n",
    "\n",
    "        # for ent in t_ents:\n",
    "        #     ner = {\n",
    "        #         'index':np.arange(ent['pos'][0][1], ent['pos'][0][2]).tolist(),\n",
    "        #         'type':ent['type']\n",
    "        #     }\n",
    "\n",
    "        #     if ent['pos'][0][0] == 1:\n",
    "        #         item1['ner'].append(ner)\n",
    "        #     elif ent['pos'][0][0] == 2:\n",
    "        #         item2['ner'].append(ner)\n",
    "        \n",
    "        # if item1['ner']:\n",
    "        #     to_save.append(item1)\n",
    "        # if item2['ner']:\n",
    "        #     to_save.append(item2)\n",
    "\n",
    "        to_save.append(item1)\n",
    "        to_save.append(item2)\n",
    "\n",
    "# np.random.seed(0)\n",
    "# np.random.shuffle(to_save)\n",
    "# np.random.shuffle(to_save)\n",
    "# n = len(to_save)\n",
    "# json.dump(to_save[:int(.8*n)], open('./data/seretod/train.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n",
    "# json.dump(to_save[int(.8*n):int(.9*n)], open('./data/seretod/dev.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n",
    "# json.dump(to_save[int(.9*n):], open('./data/seretod/test.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n",
    "json.dump(to_save, open('./data/ee_convert/test_all.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.71959678165171"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 用基线的测试方法测试此模型的输出结果\n",
    "task = 'sf'\n",
    "import json \n",
    "import re\n",
    "labels = json.load(open(f'./data/{task}/test.json', encoding='utf8'))\n",
    "bio_l = []\n",
    "texts = []\n",
    "for sent in labels:\n",
    "    texts.append(''.join(sent['sentence']))\n",
    "    res = ['O']*len(sent['sentence'])\n",
    "    for ent in sent['ner']:\n",
    "        for i in ent['index']:\n",
    "            res[i] = f'I-{ent[\"type\"]}'\n",
    "        res[ent['index'][0]] = f'B-{ent[\"type\"]}'\n",
    "\n",
    "    bio_l.append(res)\n",
    "\n",
    "output = json.load(open(f'./output/{task}/output.json', encoding='utf8'))\n",
    "bio_o = []\n",
    "for sent in output:\n",
    "    res = ['O']*len(sent['sentence'])\n",
    "    text = ''.join(sent['sentence'])\n",
    "    for ent in sent['entity']:\n",
    "        label = ''.join(ent['text'])\n",
    "        span = re.search(label, text).span()\n",
    "        for i in range(*span):\n",
    "            res[i] = f\"I-{ent['type'].upper()}\"\n",
    "        res[span[0]] = f\"B-{ent['type'].upper()}\"\n",
    "    bio_o.append(res)\n",
    "\n",
    "\n",
    "from seqeval.metrics import f1_score as span_f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "micro_f1 = span_f1_score(bio_l, bio_o, mode='strict', scheme=IOB2) * 100.0\n",
    "micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "for text, label, output in zip(texts, bio_l, bio_o):\n",
    "    if label!=output:\n",
    "        cases.append({\n",
    "            'text': text,\n",
    "            'label': ' '.join(label),\n",
    "            'output': ' '.join(output)\n",
    "        })\n",
    "json.dump(cases, open('./case.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 处理生成SF的数据集\n",
    "import numpy as np\n",
    "import json\n",
    "data = json.load(open('./data/ee/data_label.json'))\n",
    "to_save=[]\n",
    "for dialog in data:\n",
    "    for turn in dialog['content']:\n",
    "        keys = list(turn.keys())[:2]\n",
    "        t_triples = turn['info']['triples']\n",
    "        \n",
    "        item1={\n",
    "            'sentence':' '.join(turn[keys[0]]).split(),\n",
    "            'ner':[]\n",
    "        }\n",
    "        item2={\n",
    "            'sentence':' '.join(turn[keys[1]]).split(),\n",
    "            'ner':[]\n",
    "        }\n",
    "\n",
    "        for triple in t_triples:\n",
    "            ner = {\n",
    "                'index':np.arange(triple['pos'][1], triple['pos'][2]).tolist(),\n",
    "                'type':triple['prop']\n",
    "            }\n",
    "\n",
    "            if triple['pos'][0] == 1:\n",
    "                item1['ner'].append(ner)\n",
    "            elif triple['pos'][0] == 2:\n",
    "                item2['ner'].append(ner)\n",
    "        \n",
    "        if item1['ner']:\n",
    "            to_save.append(item1)\n",
    "        if item2['ner']:\n",
    "            to_save.append(item2)\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(to_save)\n",
    "np.random.shuffle(to_save)\n",
    "n = len(to_save)\n",
    "json.dump(to_save[:int(.8*n)], open('./data/sf/train.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n",
    "json.dump(to_save[int(.8*n):int(.9*n)], open('./data/sf/dev.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n",
    "json.dump(to_save[int(.9*n):], open('./data/sf/test.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n",
    "# json.dump(to_save, open('./data/all_data.json', 'w', encoding='utf8'), indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从W2NER到基线 EE\n",
    "\n",
    "import json\n",
    "before = json.load(open('/home/disk1/dgt2021/seretod/W2NER/data/ee_convert/before.json', encoding='utf8'))\n",
    "infer_res = json.load(open('/home/disk1/dgt2021/seretod/W2NER/output/ee/converted_output.json', encoding='utf8'))\n",
    "for i in range(1,20):\n",
    "    infer_res.extend(json.load(open(f\"./output/ee/converted_output{i}.json\")))\n",
    "for dialog in before:\n",
    "    for turn in dialog['content']:\n",
    "        ents = []\n",
    "        keys = list(turn.keys())\n",
    "        if turn[keys[0]] and infer_res:\n",
    "\n",
    "            spk1 = infer_res.pop(0)\n",
    "            sentence_text = ''.join(spk1['sentence'])\n",
    "            assert sentence_text == turn[keys[0]].replace(\"\\t\",\"\").replace(\" \",\"\").replace('\\x1f','').replace(\"\\u3000\",''), (sentence_text, turn, 1)\n",
    "            sentence_text = sentence_text.replace('\\x7f','')\n",
    "            turn[keys[0]] = sentence_text\n",
    "\n",
    "            recorded = []\n",
    "            for ent in spk1['entity']:\n",
    "                ent_text = ''.join(ent['text'])\n",
    "                if '\\x7f' in ent_text:\n",
    "                    continue\n",
    "                if ent_text not in recorded:\n",
    "                    begin = sentence_text.find(ent_text)\n",
    "                    ents.append({\n",
    "                        \"pos\":[[1, begin, begin+len(ent_text)]],\n",
    "                        \"type\": ent['type'],\n",
    "                        \"name\": ent_text\n",
    "                    })\n",
    "                    recorded.append(ent_text)\n",
    "\n",
    "        if turn[keys[1]] and infer_res:\n",
    "\n",
    "            spk2 = infer_res.pop(0)\n",
    "            sentence_text = ''.join(spk2['sentence'])\n",
    "            assert sentence_text == turn[keys[1]].replace(\"\\t\",\"\").replace(\" \",\"\").replace('\\x1f','').replace(\"\\u3000\",''), (sentence_text, turn, 2)\n",
    "            sentence_text = sentence_text.replace('\\x7f','')\n",
    "            turn[keys[1]] = sentence_text\n",
    "\n",
    "            recorded = []\n",
    "            for ent in spk2['entity']:\n",
    "                ent_text = ''.join(ent['text'])\n",
    "                if '\\x7f' in ent_text:\n",
    "                    continue\n",
    "                if ent_text not in recorded:\n",
    "                    begin = sentence_text.find(ent_text)\n",
    "                    ents.append({\n",
    "                        \"pos\":[[2, begin, begin+len(ent_text)]],\n",
    "                        \"type\": ent['type'],\n",
    "                        \"name\": ent_text             \n",
    "                    })\n",
    "                    recorded.append(ent_text)\n",
    "        turn[\"info\"] = {}\n",
    "        turn['info']['ents'] = ents\n",
    "    if not infer_res:\n",
    "        break\n",
    "json.dump(before, open('./data/test_with_entity_mentions.json', 'w', encoding='utf8'), ensure_ascii=False, indent=4)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 校正输出数据\n",
    "import json\n",
    "before = json.load(open('./data/test_with_entity_mentions.json', encoding='utf8'))\n",
    "id = 0\n",
    "for dialog in before:\n",
    "    # for turn in dialog['content']:\n",
    "    #     keys = list(turn.keys())\n",
    "    #     for key in keys[:2]:\n",
    "\n",
    "    #         turn[key] = turn[key].replace('\\x7f', '')\n",
    "    #     for i, ent in enumerate(turn['info']['ents']):\n",
    "    #         if ent['name']=='\\x7f':\n",
    "    #             turn['info']['ents'].pop(i)\n",
    "    dialog['id'] = f'testdata{id:0>8}'\n",
    "    id+=1\n",
    "\n",
    "json.dump(before, open('./data/test_with_entity_mentions.json', 'w', encoding='utf8'), ensure_ascii=False, indent=4)\n",
    "json.dump(before, open('/home/disk1/dgt2021/seretod/baseline/Track1/baseline/data/test_with_entity_mentions.json', 'w', encoding='utf8'), ensure_ascii=False, indent=4)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从W2NER到基线 SF\n",
    "\n",
    "import json\n",
    "before = json.load(open('/home/disk1/dgt2021/seretod/baseline/Track1/baseline/data/test_with_entity_coref.json', encoding='utf8'))\n",
    "infer_res = []\n",
    "for i in range(0,20):\n",
    "    infer_res.extend(json.load(open(f\"./output/sf/converted_output{i}.json\")))\n",
    "for dialog in before:\n",
    "    for turn in dialog['content']:\n",
    "        triples = []\n",
    "        keys = list(turn.keys())\n",
    "        if turn[keys[0]] and infer_res:\n",
    "\n",
    "            spk1 = infer_res.pop(0)\n",
    "            sentence_text = ''.join(spk1['sentence'])\n",
    "            sentence_text = sentence_text.replace('\\x7f','')\n",
    "            assert sentence_text == turn[keys[0]].replace(\"\\t\",\"\").replace(\" \",\"\").replace('\\x1f','').replace(\"\\u3000\",''), (sentence_text, turn, 1)\n",
    "            turn[keys[0]] = sentence_text\n",
    "\n",
    "            recorded = []\n",
    "            for ent in spk1['entity']:\n",
    "                ent_text = ''.join(ent['text'])\n",
    "                if '\\x7f' in ent_text:\n",
    "                    continue\n",
    "                if ent_text not in recorded:\n",
    "                    begin = sentence_text.find(ent_text)\n",
    "                    triples.append({\n",
    "                        \"pos\":[1, begin, begin+len(ent_text)],\n",
    "                        \"prop\": ent['type'],\n",
    "                        \"value\": ent_text\n",
    "                    })\n",
    "                    recorded.append(ent_text)\n",
    "\n",
    "        if turn[keys[1]] and infer_res:\n",
    "\n",
    "            spk2 = infer_res.pop(0)\n",
    "            sentence_text = ''.join(spk2['sentence'])\n",
    "            sentence_text = sentence_text.replace('\\x7f','')\n",
    "            assert sentence_text == turn[keys[1]].replace(\"\\t\",\"\").replace(\" \",\"\").replace('\\x1f','').replace(\"\\u3000\",''), (sentence_text, turn, 2)\n",
    "            turn[keys[1]] = sentence_text\n",
    "\n",
    "            recorded = []\n",
    "            for ent in spk2['entity']:\n",
    "                ent_text = ''.join(ent['text'])\n",
    "                if '\\x7f' in ent_text:\n",
    "                    continue\n",
    "                if ent_text not in recorded:\n",
    "                    begin = sentence_text.find(ent_text)\n",
    "                    triples.append({\n",
    "                        \"pos\":[2, begin, begin+len(ent_text)],\n",
    "                        \"prop\": ent['type'],\n",
    "                        \"value\": ent_text             \n",
    "                    })\n",
    "                    recorded.append(ent_text)\n",
    "        turn['info']['triples'] = triples\n",
    "    if not infer_res:\n",
    "        break\n",
    "json.dump(before, open('./data/test_with_triples.json', 'w', encoding='utf8'), ensure_ascii=False, indent=4)\n",
    "json.dump(before, open('/home/disk1/dgt2021/seretod/baseline/Track1/baseline/data/test_with_triples.json', 'w', encoding='utf8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把测试集分成小份\n",
    "import json\n",
    "import numpy as np\n",
    "data = json.load(open(\"./data/ee_convert/test_all.json\", encoding=\"utf8\"))\n",
    "n = len(data)\n",
    "split = np.linspace(0,n,21,dtype=int).tolist()\n",
    "for i in range(20):\n",
    "    json.dump(data[split[i]:split[i+1]], open(f\"./data/ee_convert/test{i}.json\", \"w\", encoding=\"utf8\"), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc00000123\n"
     ]
    }
   ],
   "source": [
    "i=123\n",
    "print(f'abc{i:0>8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('seretod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71c3eea4cf5c1ab773b4d45c468614636ae3e3c5339ee60654fd270fba2a0266"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
